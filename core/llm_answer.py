# llm_answer.py
"""
Appel au LLM pour générer une réponse finale
à partir de la question utilisateur et des chunks retenus.
Système de citations [1], [2]... pour la traçabilité des sources.
"""
from config import NUM_CHUNKS, MAX_CHUNK_LENGTH
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from config import GEN_MODEL, LLM_NUM_CTX
import os

# Permet de choisir le GPU à utiliser (par défaut GPU 0 uniquement)
def set_cuda_visible_devices(gpu_ids="0"):
    os.environ["CUDA_VISIBLE_DEVICES"] = gpu_ids

DEFAULT_SYSTEM_PROMPT = """
[ROLE] Assistant RAG Technique (FR) — Haute fiabilité, zéro hallucination, réponses explicites

[OBJECTIF]
Répondre à la QUESTION en s’appuyant EXCLUSIVEMENT sur le CONTEXTE.
Les utilisateurs posent principalement deux types de requêtes :
A) « Liste-moi toutes les exigences correspondant à l’objectif X ».
B) « Explique-moi l’exigence Y ».
Les exigences sont souvent dans des TABLEAUX. Tu dois être explicite, complet et traçable.

[PRINCIPES DURS]
1) Uniquement le CONTEXTE : aucune connaissance externe. Zéro invention.
2) Si l’information n’est pas trouvée, ambiguë ou contradictoire, répondre EXACTEMENT :
   "Je ne sais pas sur la base du contexte fourni."
3) Traçabilité : appuie chaque point clé sur des extraits EXACTS du CONTEXTE (quelques mots à une phrase) entre guillemets dans la section [Justification].
4) Tableaux : lis précisément les entêtes, lignes et unités. Conserve l’orthographe, les identifiants, les unités et l’ordre. Ne renomme pas arbitrairement.
5) Contradictions : si des données se contredisent, signale-les et n’arbitre pas sans instruction explicite.
6) Calculs/agrégations : uniquement à partir de valeurs du CONTEXTE ; montre brièvement la formule et la substitution.

[DÉTECTION D’INTENTION (INTERNE)]
- Si la QUESTION demande de « lister », « recenser », « toutes les exigences pour X », passe en MODE LISTE.
- Si la QUESTION demande « expliquer », « détailler », « clarifier » une exigence Y, passe en MODE EXPLICATION.
- Sinon, réponds simplement mais en respectant les principes ci-dessus.

[MODE LISTE — “toutes les exigences pour l’objectif X”]
But : couvrir toutes les exigences trouvées dans le CONTEXTE reliées à l’objectif X (par intitulé, colonne “Objectif”, “But”, “Requirement/Exigence”, “Critère”, “ID”, etc.).
Règles :
- Parcours des tableaux/puces/paragraphes ; sélectionne toutes les lignes/entrées qui correspondent explicitement à l’objectif X (correspondance exacte ou synonymes présents dans le CONTEXTE).
- Pour chaque exigence, restitue les champs pertinents trouvés : **ID/Code**, **Intitulé/Titre**, **Texte de l’exigence**, **Conditions/Portée**, **Valeurs/Seuils/Unités**, **Notes/Exceptions**… uniquement si présents dans le CONTEXTE.
- Si une ligne de tableau correspond, privilégie la **restitution de la ligne complète** (colonnes → valeurs).
- Déduplication : si plusieurs occurrences de la même exigence existent, fusionne-les prudemment en conservant les variantes et en les notant.
- Ordre : numérote et garde l’ordre logique du document (ou l’ordre d’apparition).
- Si aucune exigence ne correspond : renvoie la phrase standard “Je ne sais pas…” ci-dessus.

Sortie MODE LISTE (exemple de structure) :
[Réponse]
1) ID: … | Intitulé: … 
   Exigence: … 
   Conditions/Portée: … 
   Valeurs/Seuils: … 
   Notes: …
2) …

[Justification]
- "…extrait exact lié à l’objectif X…"
- "…extrait exact de la ligne/colonne…"
- (autant que nécessaire, citations courtes et précises)

[MODE EXPLICATION — “expliquer l’exigence Y”]
But : produire une explication technique, fidèle et opérationnelle à partir du CONTEXTE.
Règles :
- Identifier l’exigence (ID/Intitulé/ligne de tableau) dans le CONTEXTE.
- Expliquer : **définition**, **but/objectif** (uniquement s’il est mentionné), **conditions/portée**, **valeurs/contraintes/limites** (avec unités), **exceptions**, **dépendances/prérequis**, **procédure** si applicable.
- Si l’exigence est présentée dans un tableau, restituer les colonnes pertinentes (ID, Description, Critère, Seuil, Unité, Mode, etc.).
- Si la QUESTION demande des exemples et que des exemples sont présents dans le CONTEXTE, les inclure tels quels (extraits).
- Ne pas extrapoler au-delà du CONTEXTE.

Comment est-ce que tu dois réfléchir : la Sortie du MODE EXPLICATION (exemple) :
[Réponse]
- Définition: …
- Portée/Conditions: …
- Valeurs/Seuils (avec unités): …
- Exceptions/Notes: …
- Procédure/Règles d’application: …
- Exemple(s) présent(s) dans le CONTEXTE: …

[Justification]
- "…extrait exact 1…"
- "…extrait exact 2…"
- (références textuelles courtes : ligne/colonne si déductible du texte)

(n'affiche pas le mode explication, ce mode doit servir de base de connaissance pour avoir une meilleure réponse finale.)

[COMPORTEMENT EN CAS D’AMBIGUÏTÉ OU DE MANQUE]
- Si correspondances partielles (p. ex. l’objectif X n’apparaît qu’en partie ou via un synonyme explicite dans le CONTEXTE), expliquer prudemment et citer l’extrait exact justifiant le lien.
- Si données manquantes (ex. seuil sans unité), le signaler explicitement dans [Réponse] et [Justification].
- Si rien de suffisamment clair : répondre "Je ne sais pas sur la base du contexte fourni."

[FORMAT FINAL — TOUJOURS]
[Réponse]
réponse finale très explicite, détaillé si il le faut, exhaustive pour la question qui est demandé, sans contenu hors CONTEXTE
elle doit être la réponse finale donc ce que l'utilisateur lit, comprends, interprête, c'est la partie la plus importante du processus.
Utilise les numéros de source [1], [2], etc. pour indiquer d'où provient chaque information dans ta réponse.

[Justification]
- Précise où tu es allé chercher la ou les informations pour répondre, en citant les numéros de source [1], [2], etc.

""".strip()

def get_system_prompt():
    """Charge le prompt système depuis la session ou retourne le default."""
    return DEFAULT_SYSTEM_PROMPT


def _chunk_source_label(chunk: dict, idx: int) -> str:
    """
    Construit le label de source d'un chunk :  [1] Source: fichier.md, page 5
    Si pas de page_number, affiche juste la source.
    """
    meta = chunk.get("meta", {})
    source = meta.get("source", "inconnu")
    page = meta.get("page_number")
    label = f"[{idx}] Source: {source}"
    if page is not None:
        label += f", page {page}"
    return label


def build_context(chunks, max_chars=NUM_CHUNKS * MAX_CHUNK_LENGTH) -> str:
    """
    Construit le contexte en numérotant chaque chunk [1], [2], ...
    avec sa source et son numéro de page.
    Coupe si ça dépasse `max_chars`.
    """
    parts = []
    for i, c in enumerate(chunks, start=1):
        doc = c.get("doc", "").strip()
        if not doc:
            continue
        label = _chunk_source_label(c, i)
        parts.append(f"{label}\n{doc}")

    context = "\n\n---\n\n".join(parts)
    if len(context) > max_chars:
        context = context[:max_chars] + "\n\n[Contexte tronqué]"
    return context


def build_citation_map(chunks) -> list[dict]:
    """
    Construit la table de correspondance citation -> source pour l'affichage UI.
    Retourne une liste de dicts avec toutes les infos de traçabilité.
    """
    citations = []
    for i, c in enumerate(chunks, start=1):
        meta = c.get("meta", {})
        citations.append({
            "idx": i,
            "source": meta.get("source", "inconnu"),
            "page": meta.get("page_number"),
            "section": meta.get("section_idx"),
            "chunk": meta.get("chunk_idx"),
            "heading": meta.get("heading", ""),
            "breadcrumb": meta.get("breadcrumb", ""),
            "chunk_type": meta.get("chunk_type", "chunk"),
        })
    return citations


def _build_history_block(history: list[dict]) -> str:
    """
    Formate l'historique conversationnel pour l'injection dans le prompt.
    history = [{"role": "user"|"assistant", "content": "..."}]
    Ne garde que les 6 derniers échanges (3 tours) pour rester dans la fenêtre de contexte.
    """
    if not history:
        return ""
    recent = history[-6:]
    lines = ["[HISTORIQUE DE LA CONVERSATION]"]
    for msg in recent:
        role = "Utilisateur" if msg["role"] == "user" else "Assistant"
        lines.append(f"{role}: {msg['content'][:800]}")  # tronque les longs messages
    lines.append("[FIN DE L'HISTORIQUE]")
    return "\n".join(lines)


def answer(question: str, chunks: list[dict], gpu_ids="0", system_prompt=None,
           conversation_history: list[dict] = None) -> tuple[str, list[dict]]:
    """
    Prend la question utilisateur + les chunks sélectionnés,
    envoie un prompt au LLM, retourne (réponse_texte, citation_map).
    conversation_history : liste de {"role": "user"|"assistant", "content": "..."} 
                           pour la mémoire conversationnelle.
    """
    set_cuda_visible_devices(gpu_ids)
    context = build_context(chunks)
    citations = build_citation_map(chunks)

    # Utilise le system_prompt fourni ou le default
    if system_prompt is None:
        system_prompt = get_system_prompt()

    history_block = _build_history_block(conversation_history or [])

    llm = Ollama(
        model=GEN_MODEL,      # défini dans config.py
        temperature=0.3,
        top_k=NUM_CHUNKS,
        top_p=0.8,
        repeat_penalty=1.5,
        num_ctx=LLM_NUM_CTX,  # fenêtre de contexte explicite (évite 32k+ par défaut)
    )

    history_section = f"\n\n{history_block}\n" if history_block else ""
    template = system_prompt + history_section + """

CONTEXTE :
{context}

Question :
{question}

Réponse :"""

    prompt_fr = PromptTemplate(
        input_variables=["context", "question"],
        template=template.strip()
    )

    final_prompt = prompt_fr.format(context=context, question=question)
    response = llm.invoke(final_prompt)
    return response, citations


def answer_stream(question: str, chunks: list[dict], gpu_ids="0", system_prompt=None,
                  conversation_history: list[dict] = None):
    """
    Version streaming de answer() : génère les tokens un par un via llm.stream().
    Retourne (générateur, citations).
    conversation_history : mémoire conversationnelle injectée dans le prompt.
    """
    set_cuda_visible_devices(gpu_ids)
    context = build_context(chunks)
    citations = build_citation_map(chunks)

    if system_prompt is None:
        system_prompt = get_system_prompt()

    history_block = _build_history_block(conversation_history or [])

    llm = Ollama(
        model=GEN_MODEL,
        temperature=0.3,
        top_k=NUM_CHUNKS,
        top_p=0.8,
        repeat_penalty=1.5,
        keep_alive=-1,          # garder le modèle en VRAM entre les appels
        num_ctx=LLM_NUM_CTX,    # fenêtre de contexte explicite (évite 32k+ par défaut)
    )

    history_section = f"\n\n{history_block}\n" if history_block else ""
    template = system_prompt + history_section + """

CONTEXTE :
{context}

Question :
{question}

Réponse :"""

    prompt_fr = PromptTemplate(
        input_variables=["context", "question"],
        template=template.strip()
    )

    final_prompt = prompt_fr.format(context=context, question=question)

    def _gen():
        try:
            for chunk in llm.stream(final_prompt):
                if chunk:
                    yield chunk
        except (BrokenPipeError, ConnectionResetError, GeneratorExit):
            # Connexion Ollama coupée (ex: re-render Streamlit) — on arrête proprement
            return
        except Exception as e:
            # Tout autre erreur réseau/LLM : on log et on sort
            print(f"[answer_stream] Erreur pendant le stream : {e}")
            return

    gen = _gen()
    return gen, citations
