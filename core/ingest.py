# ingest.py
from pathlib import Path
import pickle

from indexing.chunking import decoupe_semantic_md
from indexing.embedding import index_chroma, build_embeddings
from indexing.keyword_index import build_bm25_index, save_bm25_to_mongo
from config import (COLLECTION_NAME, AUTO_KEYWORDS, AUTO_QUESTIONS,
                    ENHANCEMENT_MODEL, CHUNKING_MODE,
                    RAPTOR_SUMMARIES, RAPTOR_MIN_CHUNKS, RAPTOR_MAX_INPUT_CHUNKS)

from nlp.vocab_builder import save_vocab, build_vocab
from nlp.chunk_enhancer import enhance_chunks, build_raptor_summaries
from nlp.graph_builder import build_entity_graph, save_graph_to_mongo, graph_stats
from indexing.store_mongo import save_chunks_to_mongo


def print_chunk_stats(docs):
    # Affiche un résumé par section si présent
    section_counts = {}
    for d in docs:
        sec = d.metadata.get("section_idx", "?")
        section_counts[sec] = section_counts.get(sec, 0) + 1
    print("\nChunks par section Markdown :")
    for sec, count in sorted(section_counts.items()):
        print(f"  Section {sec}: {count} chunks")


def ingest_markdown(md_path: str, output_dir: str = "./data/vocab_save",
                    num_keywords: int = None, num_questions: int = None,
                    enhancement_model: str = None, chunking_mode: str = None,
                    raptor_summaries: bool = None,
                    progress_callback=None):
    """
    Pipeline d'ingestion complet :
    - Découpe en chunks (mode naive ou technical)
    - Enrichissement LLM (keywords + questions) si activé
    - Résumés RAPTOR par section si activé
    - Construction du vocabulaire
    - Embeddings avec Ollama
    - Indexation Chroma
    - Sauvegarde MongoDB
    - Sauvegarde BM25
    
    Args:
        md_path: chemin du fichier .md
        output_dir: dossier de sortie vocabulaire
        num_keywords: nombre de mots-clés par chunk (None = config AUTO_KEYWORDS)
        num_questions: nombre de questions par chunk (None = config AUTO_QUESTIONS)
        enhancement_model: modèle LLM pour l'enrichissement (None = config)
        chunking_mode: mode de chunking "naive" ou "technical" (None = config CHUNKING_MODE)
        raptor_summaries: activer les résumés RAPTOR (None = config RAPTOR_SUMMARIES)
        progress_callback: callable(step_name, progress_pct) pour UI
    
    Retourne un dictionnaire avec les stats d'ingestion.
    """
    # Paramètres par défaut depuis config
    if num_keywords is None:
        num_keywords = AUTO_KEYWORDS
    if num_questions is None:
        num_questions = AUTO_QUESTIONS
    if enhancement_model is None:
        enhancement_model = ENHANCEMENT_MODEL
    if chunking_mode is None:
        chunking_mode = CHUNKING_MODE
    if raptor_summaries is None:
        raptor_summaries = RAPTOR_SUMMARIES
    
    stats = {}
    
    try:
        # 1) Vérifier que le fichier existe
        if not Path(md_path).exists():
            raise FileNotFoundError(f"Fichier {md_path} introuvable")
        
        if progress_callback:
            progress_callback("Découpe en chunks...", 5)
        
        # 2) Découper en chunks
        docs = decoupe_semantic_md(md_path, max_characters=1000, mode=chunking_mode)
        stats["num_chunks"] = len(docs)
        stats["chunking_mode"] = chunking_mode
        print(f"{len(docs)} chunks générés à partir de {md_path} (mode: {chunking_mode})")
        print_chunk_stats(docs)
        stats["chunks_per_section"] = {}
        for d in docs:
            sec = d.metadata.get("section_idx", "?")
            stats["chunks_per_section"][sec] = stats["chunks_per_section"].get(sec, 0) + 1
        
        # 3) Enrichissement LLM (keywords + questions)
        if num_keywords > 0 or num_questions > 0:
            if progress_callback:
                progress_callback("Enrichissement LLM (keywords + questions)...", 15)
            
            def _enhance_progress(current, total):
                # Progression de 15% à 50% pendant l'enrichissement
                pct = 15 + int(35 * current / total)
                if progress_callback:
                    progress_callback(f"Enrichissement chunk {current}/{total}...", pct)
            
            docs = enhance_chunks(
                docs,
                num_keywords=num_keywords,
                num_questions=num_questions,
                model=enhancement_model,
                progress_callback=_enhance_progress
            )
            chunks_with_table_desc = len([d for d in docs if d.metadata.get("table_description")])
            chunks_with_entities = len([d for d in docs if d.metadata.get("entities_str")])
            stats["enhancement"] = {
                "num_keywords": num_keywords,
                "num_questions": num_questions,
                "chunks_enhanced": len([d for d in docs if d.metadata.get("keywords")]),
                "table_descriptions": chunks_with_table_desc,
                "chunks_with_entities": chunks_with_entities
            }
            print(f"Enrichissement terminé : {stats['enhancement']['chunks_enhanced']} chunks enrichis")
            if chunks_with_table_desc:
                print(f"  → {chunks_with_table_desc} descriptions de tableaux générées")
            print(f"  → {chunks_with_entities} chunks avec entités nommées")
        else:
            stats["enhancement"] = {"num_keywords": 0, "num_questions": 0, "chunks_enhanced": 0}
            # S'assurer que les métadonnées existent même sans enrichissement
            for d in docs:
                d.metadata.setdefault("keywords", [])
                d.metadata.setdefault("keywords_str", "")
                d.metadata.setdefault("questions", [])
                d.metadata.setdefault("questions_str", "")

        # Étape 7 : NER sur tous les chunks (même si enrichissement LLM désactivé)
        # Le NER spaCy est rapide (~0.5ms/chunk) donc on le fait toujours
        from nlp.ner_extractor import extract_entities, entities_to_str, entities_to_flat_list
        for d in docs:
            if not d.metadata.get("entities_str"):
                ner_dict = extract_entities(d.page_content)
                d.metadata.setdefault("entities", ner_dict)
                d.metadata.setdefault("entities_flat", entities_to_flat_list(ner_dict))
                d.metadata.setdefault("entities_str", entities_to_str(ner_dict))
        
        # Marquer tous les chunks comme type "chunk" (par défaut)
        for d in docs:
            d.metadata.setdefault("chunk_type", "chunk")
        
        # 3b) Résumés RAPTOR par section
        if raptor_summaries:
            if progress_callback:
                progress_callback("Génération des résumés RAPTOR par section...", 50)
            
            def _raptor_progress(current, total):
                pct = 50 + int(5 * current / max(total, 1))
                if progress_callback:
                    progress_callback(f"Résumé RAPTOR {current}/{total}...", pct)
            
            summary_docs = build_raptor_summaries(
                docs,
                min_chunks=RAPTOR_MIN_CHUNKS,
                max_input_chunks=RAPTOR_MAX_INPUT_CHUNKS,
                model=enhancement_model,
                progress_callback=_raptor_progress
            )
            stats["raptor"] = {
                "enabled": True,
                "summaries_generated": len(summary_docs),
            }
            # Ajouter les résumés à la liste des documents
            docs.extend(summary_docs)
            print(f"RAPTOR : {len(summary_docs)} résumés ajoutés → {len(docs)} docs total")
        else:
            stats["raptor"] = {"enabled": False, "summaries_generated": 0}
        
        if progress_callback:
            progress_callback("Construction du vocabulaire...", 55)
        
        # 4) Construction du vocabulaire
        vocab, acronyms = build_vocab(docs, top_k_terms=3000)
        output_dir_path = Path(output_dir)
        output_dir_path.mkdir(parents=True, exist_ok=True)
        save_vocab(vocab, acronyms, path=str(output_dir_path / "vocab.json"))
        stats["vocab_size"] = len(vocab)
        stats["acronyms_count"] = len(acronyms)
        print("Vocabulaire corpus sauvegardé")
        
        if progress_callback:
            progress_callback("Génération des embeddings...", 60)
        
        # 5) Embeddings via Ollama
        texts, embeddings, metadatas, ids = build_embeddings(docs)
        stats["embeddings_count"] = len(embeddings)
        print("Embeddings générés")
        
        if progress_callback:
            progress_callback("Indexation ChromaDB...", 75)
        
        # 6) Indexation Chroma
        _ = index_chroma(ids, texts, metadatas, embeddings, collection_name=COLLECTION_NAME)
        print("Indexation Chroma terminée")
        
        if progress_callback:
            progress_callback("Sauvegarde MongoDB...", 85)
        
        # 7) Sauvegarde MongoDB
        save_chunks_to_mongo(docs)
        print("Chunks sauvegardés dans MongoDB")
        
        if progress_callback:
            progress_callback("Construction index BM25...", 88)
        
        # 8) Index BM25 (enrichi avec keywords+questions)
        bm25_tuple = build_bm25_index(docs)
        # Sauvegarde MongoDB (multi-document)
        save_bm25_to_mongo(bm25_tuple, source_doc=Path(md_path).name)
        # Sauvegarde .pkl (fallback global — conservé pour compatibilité)
        _bm25_pkl = Path(__file__).resolve().parent.parent / "data" / "bm25_index.pkl"
        _bm25_pkl.parent.mkdir(parents=True, exist_ok=True)
        with open(_bm25_pkl, "wb") as f:
            pickle.dump(bm25_tuple, f)
        print("Index BM25 sauvegardé (MongoDB + pkl)")

        # 9) Étape 8 : Construction du graphe d'entités (GraphRAG)
        if progress_callback:
            progress_callback("Construction du graphe d'entites (GraphRAG)...", 92)

        def _graph_progress(current, total):
            pct = 92 + int(6 * current / max(total, 1))
            if progress_callback:
                progress_callback(f"GraphRAG {current}/{total}...", pct)

        entity_graph = build_entity_graph(
            docs,
            use_llm_relations=True,
            model=enhancement_model,
            progress_callback=_graph_progress
        )
        save_graph_to_mongo(entity_graph, source_doc=Path(md_path).name)
        g_stats = graph_stats(entity_graph)
        stats["graph"] = g_stats
        print(f"GraphRAG : {g_stats['nodes']} entites, {g_stats['edges']} relations")
        
        if progress_callback:
            progress_callback("Termine", 100)
        
        stats["status"] = "success"
        stats["message"] = f"Ingestion terminée : {len(docs)} chunks indexés"
        
    except Exception as e:
        stats["status"] = "error"
        stats["message"] = str(e)
        print(f"Erreur lors de l'ingestion : {e}")
    
    return stats

if __name__ == "__main__":
    md_file = input("Nom du fichier .md nettoyé à utiliser : ").strip()
    if not Path(md_file).exists():
        print(f"Erreur : fichier {md_file} introuvable")
        exit(1)

    ingest_markdown(md_file)

